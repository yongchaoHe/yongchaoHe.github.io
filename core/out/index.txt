1:"$Sreact.fragment"
2:I[3719,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-7eaa2dfd13e092db.js"],"ThemeProvider"]
3:I[768,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-7eaa2dfd13e092db.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["17","static/chunks/17-9c04c9413a9f9f1f.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-7eaa2dfd13e092db.js"],"default"]
7:I[7437,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-2db3a9d7664fb123.js","974","static/chunks/app/page-108ef18e3cc7ad43.js"],"default"]
8:I[9507,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-2db3a9d7664fb123.js","974","static/chunks/app/page-108ef18e3cc7ad43.js"],"default"]
a:I[5218,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-2db3a9d7664fb123.js","974","static/chunks/app/page-108ef18e3cc7ad43.js"],"default"]
15:I[1990,["17","static/chunks/17-9c04c9413a9f9f1f.js","178","static/chunks/178-595a94b9af1e67b5.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-2db3a9d7664fb123.js","974","static/chunks/app/page-108ef18e3cc7ad43.js"],"default"]
16:I[9665,[],"MetadataBoundary"]
18:I[9665,[],"OutletBoundary"]
1b:I[4911,[],"AsyncMetadataOutlet"]
1d:I[9665,[],"ViewportBoundary"]
1f:I[6614,[],""]
:HL["/_next/static/css/f6c8eb37d788b6a5.css","style"]
9:T434,Welcome to Yongchao He's homepage!

I‚Äôm a researcher and engineer working on LLM systems and AI infrastructure.
I build systems that keep large models alive ‚Äî and sometimes make them faster, from distributed training to LLM inference and all the little optimizations that help big things scale.

My work sits at the intersection of distributed systems, runtime design, and large-scale LLM training and inference, with a focus on system-level abstractions, performance trade-offs, and system‚Äìmodel co-design.

I earned my Ph.D. from the [Institute for Interdisciplinary Information Sciences](https://iiis.tsinghua.edu.cn/en/) (IIIS, ‰∫§Âèâ‰ø°ÊÅØÁ†îÁ©∂Èô¢) at Tsinghua University, where I was advised by [Wei Xu](https://people.iiis.tsinghua.edu.cn/\~weixu/) (ÂæêËë≥) and [Wenfei Wu](https://wenfei-wu.github.io) (Âê¥ÊñáÊñê).

Now, I lead a research & develop group focused on AI infra ‚Äî exploring how to make large-scale AI systems faster, cheaper, and a little less painful.

Occasional cat wrangler, full-time system tinkerer.
Just another small node in the AI era.

b:T580,Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present L4, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. L4 partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. L4 devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, L4 reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems.c:T701,@misc{yuan2025l4lowlatencyloadbalancedllm,
  title = {L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling},
  author = {Yitao Yuan and Chenqi Zhao and Bohan Zhao and Zane Cao and Yongchao He and Wenfei Wu},
  year = {2025},
  eprint = {2512.19179},
  archivePrefix = {arXiv},
  primaryClass = {cs.DC},
  abstract = {Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present L4, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. L4 partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. L4 devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, L4 reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems.},
  url = {https://arxiv.org/abs/2512.19179}
}d:T539,Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving ‚â• 99% of full-attention accuracy and up to 2.61√ó faster attention computation than FlashAttention.e:T689,@misc{liu2025unifiedsparseattentionmultigranularity,
  title = {A Unified Sparse Attention via Multi-Granularity Compression},
  author = {Siran Liu and Zane Cao and Yongchao He},
  year = {2025},
  eprint = {2512.14082},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  abstract = {Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving ‚â• 99% of full-attention accuracy and up to 2.61√ó faster attention computation than FlashAttention.},
  url = {https://arxiv.org/abs/2512.14082}
}f:T43e,As inference workloads for large language models (LLMs) scale to meet growing user demand, pipeline parallelism (PP) has become a widely adopted strategy for multi-GPU deployment, particularly in cross-node setups, to improve key-value (KV) cache capacity and inference throughput. However, PP suffers from inherent inefficiencies caused by three types of execution bubbles-load-imbalance, intra-stage, and inter-stage-which limit pipeline saturation. We present SiPipe, a heterogeneous pipeline design that improves throughput by leveraging underutilized CPU resources to offload auxiliary computation and communication. SiPipe incorporates three key techniques-CPU sampling, a token-safe execution model, and structure-aware transmission-to mitigate pipeline bubbles and improve execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1 times higher throughput, 43% lower per-token latency, and up to 23% higher average GPU utilization compared to the state-of-the-art vLLM under the same PP configuration, demonstrating its generality across LLMs and deployment scenarios.10:T5a6,@misc{he2025sipipebridgingcpugpuutilization,
  title = {SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference},
  author = {Yongchao He and Bohan Zhao and Zheng Cao},
  year = {2025},
  eprint = {2506.22033},
  archivePrefix = {arXiv},
  primaryClass = {cs.DC},
  abstract = {As inference workloads for large language models (LLMs) scale to meet growing user demand, pipeline parallelism (PP) has become a widely adopted strategy for multi-GPU deployment, particularly in cross-node setups, to improve key-value (KV) cache capacity and inference throughput. However, PP suffers from inherent inefficiencies caused by three types of execution bubbles-load-imbalance, intra-stage, and inter-stage-which limit pipeline saturation. We present SiPipe, a heterogeneous pipeline design that improves throughput by leveraging underutilized CPU resources to offload auxiliary computation and communication. SiPipe incorporates three key techniques-CPU sampling, a token-safe execution model, and structure-aware transmission-to mitigate pipeline bubbles and improve execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1 times higher throughput, 43% lower per-token latency, and up to 23% higher average GPU utilization compared to the state-of-the-art vLLM under the same PP configuration, demonstrating its generality across LLMs and deployment scenarios.},
  url = {https://arxiv.org/abs/2506.22033}
}11:T544,Autoregressive decoding inherently limits the inference throughput of Large Language Model (LLM) due to its sequential dependency. Speculative decoding mitigates this by verifying multiple predicted tokens in parallel, but its efficiency remains constrained by what we identify as verification heterogeneity -- the uneven difficulty of verifying different speculative candidates. In practice, a small subset of high-confidence predictions accounts for most successful verifications, yet existing methods treat all candidates uniformly, leading to redundant computation. We present HeteroSpec, a heterogeneity-adaptive speculative decoding framework that allocates verification effort in proportion to candidate uncertainty. HeteroSpec estimates verification complexity using a lightweight entropy-based quantifier, partitions candidates via a data-driven stratification policy, and dynamically tunes speculative depth and pruning thresholds through coordinated optimization. Across five benchmarks and four LLMs, HeteroSpec delivers an average 4.24√ó decoding speedup over state-of-the-art methods such as EAGLE-3, while preserving exact output distributions. Crucially, HeteroSpec requires no model retraining and remains compatible with other inference optimizations, making it a practical direction for improving speculative decoding efficiency.12:T6cc,@misc{liu2025heterospecleveragingcontextualheterogeneity,
  title = {HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding},
  author = {Siran Liu and Yang Ye and Qianchao Zhu and Zane Cao and Yongchao He},
  year = {2025},
  eprint = {2505.13254},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  abstract = {Autoregressive decoding inherently limits the inference throughput of Large Language Model (LLM) due to its sequential dependency. Speculative decoding mitigates this by verifying multiple predicted tokens in parallel, but its efficiency remains constrained by what we identify as verification heterogeneity -- the uneven difficulty of verifying different speculative candidates. In practice, a small subset of high-confidence predictions accounts for most successful verifications, yet existing methods treat all candidates uniformly, leading to redundant computation. We present HeteroSpec, a heterogeneity-adaptive speculative decoding framework that allocates verification effort in proportion to candidate uncertainty. HeteroSpec estimates verification complexity using a lightweight entropy-based quantifier, partitions candidates via a data-driven stratification policy, and dynamically tunes speculative depth and pruning thresholds through coordinated optimization. Across five benchmarks and four LLMs, HeteroSpec delivers an average 4.24√ó decoding speedup over state-of-the-art methods such as EAGLE-3, while preserving exact output distributions. Crucially, HeteroSpec requires no model retraining and remains compatible with other inference optimizations, making it a practical direction for improving speculative decoding efficiency.},
  url = {https://arxiv.org/abs/2505.13254}
}13:T503,Key-value stream aggregation is a common operation in distributed systems, which requires intensive computation and network resources. We propose a generic in-network aggregation service for key-value streams, ASK, to accelerate the aggregation operations in diverse distributed applications. ASK is a switch-host co-designed system, where the programmable switch provides a best-effort aggregation service, and the host runs a daemon to interact with applications. ASK makes in-depth optimization tailored to traffic characteristics, hardware restrictions, and network unreliable natures: it vectorizes multiple key-value tuples‚Äô aggregation of one packet in one switch pipeline pass, which improves the per-host‚Äôs goodput; it develops a lightweight reliability mechanism for key-value stream‚Äôs asynchronous aggregation, which guarantees computation correctness; it designs a hot-key agnostic prioritization for key-skewed workloads, which improves the switch memory utilization. We prototype ASK and use it to support Spark and BytePS. The evaluation shows that ASK could accelerate pure key-value aggregation tasks by up to 155 times and big data jobs by 3-5 times, and be backward compatible with existing INA-empowered distributed training solutions with the same speedup.14:T7ba,@inproceedings{10.1145/3575693.3575708,
  author = {He, Yongchao and Wu, Wenfei and Le, Yanfang and Liu, Ming and Lao, ChonLam},
  title = {A Generic Service to Provide In-Network Aggregation for Key-Value Streams},
  year = {2023},
  isbn = {9781450399166},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3575693.3575708},
  doi = {10.1145/3575693.3575708},
  abstract = {Key-value stream aggregation is a common operation in distributed systems, which requires intensive computation and network resources. We propose a generic in-network aggregation service for key-value streams, ASK, to accelerate the aggregation operations in diverse distributed applications. ASK is a switch-host co-designed system, where the programmable switch provides a best-effort aggregation service, and the host runs a daemon to interact with applications. ASK makes in-depth optimization tailored to traffic characteristics, hardware restrictions, and network unreliable natures: it vectorizes multiple key-value tuples‚Äô aggregation of one packet in one switch pipeline pass, which improves the per-host‚Äôs goodput; it develops a lightweight reliability mechanism for key-value stream‚Äôs asynchronous aggregation, which guarantees computation correctness; it designs a hot-key agnostic prioritization for key-skewed workloads, which improves the switch memory utilization. We prototype ASK and use it to support Spark and BytePS. The evaluation shows that ASK could accelerate pure key-value aggregation tasks by up to 155 times and big data jobs by 3-5 times, and be backward compatible with existing INA-empowered distributed training solutions with the same speedup.},
  booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages = {33‚Äì47},
  numpages = {15},
  location = {Vancouver, BC, Canada},
  series = {ASPLOS 2023}
}0:{"P":null,"b":"EYPp9mNHVxrfq0KrE1BIC","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/f6c8eb37d788b6a5.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/leaf.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Projects","type":"page","target":"projects","href":"/projects"},{"title":"Awards","type":"page","target":"awards","href":"/awards"},{"title":"Services","type":"page","target":"services","href":"/services"}],"siteTitle":"Yongchao He","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"Dec 27, 2025"}]]}]}]]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-4 gap-16","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$L7",null,{"author":{"name":"Yongchao He","title":"Cats. Code. Chaos.","avatar":"/avatar.jpeg"},"social":{"email":"yongchao-he@outlook.com","google_scholar":"https://scholar.google.com/citations?hl=en&user=1vKOfVkAAAAJ&view_op=list_works&sortby=pubdate","github":"https://github.com/yongchaoHe"},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["LLM Systems (Inference & Training)","Systems Architecture & Optimization","Cat Taming","Cooking"]}]}],["$","div",null,{"className":"lg:col-span-3 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$L8","about",{"content":"$9","title":"About"}],["$","$La","featured_publications",{"publications":[{"id":"yuan2025l4lowlatencyloadbalancedllm","title":"L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling","authors":[{"name":"Yitao Yuan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chenqi Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bohan Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zane Cao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yongchao He","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Wenfei Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"preprint","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags","researchArea":"machine-learning","journal":"","conference":"","url":"https://arxiv.org/abs/2512.19179","abstract":"$b","description":"","selected":true,"preview":"L4.png","bibtex":"$c"},{"id":"liu2025unifiedsparseattentionmultigranularity","title":"A Unified Sparse Attention via Multi-Granularity Compression","authors":[{"name":"Siran Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zane Cao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yongchao He","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"preprint","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags","researchArea":"transformer-architectures","journal":"","conference":"","url":"https://arxiv.org/abs/2512.14082","abstract":"$d","description":"","selected":true,"preview":"Unisparse.png","bibtex":"$e"},{"id":"he2025sipipebridgingcpugpuutilization","title":"SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference","authors":[{"name":"Yongchao He","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Bohan Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zheng Cao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"preprint","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"","url":"https://arxiv.org/abs/2506.22033","abstract":"$f","description":"","selected":true,"preview":"sipipe.png","bibtex":"$10"},{"id":"liu2025heterospecleveragingcontextualheterogeneity","title":"HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding","authors":[{"name":"Siran Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yang Ye","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qianchao Zhu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zane Cao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yongchao He","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"preprint","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:3:tags","researchArea":"machine-learning","journal":"","conference":"","url":"https://arxiv.org/abs/2505.13254","abstract":"$11","description":"","selected":true,"preview":"heterospec.png","bibtex":"$12"},{"id":"10.1145/3575693.3575708","title":"A Generic Service to Provide In-Network Aggregation for Key-Value Streams","authors":[{"name":"Yongchao He","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Wenfei Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yanfang Le","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ming Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"ChonLam Lao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"conference","status":"published","tags":["Big Data","In-Network Aggregation","Key-Value","P4"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:4:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems","pages":"33‚Äì47","doi":"10.1145/3575693.3575708","url":"https://doi.org/10.1145/3575693.3575708","abstract":"$13","description":"","selected":true,"preview":"ASK.png","bibtex":"$14"}],"title":"Selected Publications","enableOnePageMode":false}],["$","$L15","news",{"items":[{"date":"2024-12","content":"I joined ScitiX AI as a founding member. üöÄ"},{"date":"2024-06","content":"I founded and lead the AI Infrastructure R&D group at UbiQuant. ‚öôÔ∏è"},{"date":"2024-03","content":"Selected as the first member of UbiQuant‚Äôs CTO List, a high-potential technical leadership program. üèÜ"},{"date":"2023-07","content":"I started my position at UbiQuant under the Wutong Program (Ê¢ßÊ°êËÆ°Âàí), a premier Technical Talent Program. üöÄ"},{"date":"2023-06","content":"I earned my Ph.D. in Computer Science üéì"},{"date":"2023-03","content":"Our paper titled 'A Generic Service to Provide In-Network Aggregation for Key-Value Streams' received the Distinguished Paper Award at ASPLOS 2023. üèÜ"}],"title":"News"}]],false,false,false]}]]}]]}]}],["$","$L16",null,{"children":"$L17"}],null,["$","$L18",null,{"children":["$L19","$L1a",["$","$L1b",null,{"promise":"$@1c"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","ORbKsmVkIu222Z5OUDKYV",{"children":[["$","$L1d",null,{"children":"$L1e"}],null]}],null]}],false]],"m":"$undefined","G":["$1f","$undefined"],"s":false,"S":true}
20:"$Sreact.suspense"
21:I[4911,[],"AsyncMetadata"]
17:["$","$20",null,{"fallback":null,"children":["$","$L21",null,{"promise":"$@22"}]}]
1a:null
1e:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
19:null
22:{"metadata":[["$","title","0",{"children":"Yongchao He"}],["$","meta","1",{"name":"description","content":"PhD student at the University of Example."}],["$","meta","2",{"name":"author","content":"Yongchao He"}],["$","meta","3",{"name":"keywords","content":"Yongchao He,PhD,Research,"}],["$","meta","4",{"name":"creator","content":"Yongchao He"}],["$","meta","5",{"name":"publisher","content":"Yongchao He"}],["$","meta","6",{"property":"og:title","content":"Yongchao He"}],["$","meta","7",{"property":"og:description","content":"PhD student at the University of Example."}],["$","meta","8",{"property":"og:site_name","content":"Yongchao He's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Yongchao He"}],["$","meta","13",{"name":"twitter:description","content":"PhD student at the University of Example."}],["$","link","14",{"rel":"icon","href":"/leaf.svg"}]],"error":null,"digest":"$undefined"}
1c:{"metadata":"$22:metadata","error":null,"digest":"$undefined"}
