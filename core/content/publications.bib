@misc{yuan2025l4lowlatencyloadbalancedllm,
      selected={true},
      title={L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling}, 
      author={Yitao Yuan and Chenqi Zhao and Bohan Zhao and Zane Cao and Yongchao He and Wenfei Wu},
      year={2025},
      eprint={2512.19179},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      abstract = {Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present L4, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. L4 partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. L4 devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, L4 reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems.},
      url={https://arxiv.org/abs/2512.19179}, 
      preview={L4.pdf}
}
@misc{liu2025unifiedsparseattentionmultigranularity,
      selected={true},
      title={A Unified Sparse Attention via Multi-Granularity Compression}, 
      author={Siran Liu and Zane Cao and Yongchao He},
      year={2025},
      eprint={2512.14082},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abstract={Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving ≥ 99% of full-attention accuracy and up to 2.61× faster attention computation than FlashAttention.},
      url={https://arxiv.org/abs/2512.14082}, 
      preview={Unisparse.pdf}
}
@misc{zhao2025simpledisaggregatingsamplinggpu,
      title={SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving}, 
      author={Bohan Zhao and Zane Cao and Yongchao He},
      year={2025},
      eprint={2512.00719},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      abstract={As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.},
      url={https://arxiv.org/abs/2512.00719}, 
      preview={simple.pdf}
}
@misc{zhao2025megatronappefficientcomprehensivemanagement,
      title={MegatronApp: Efficient and Comprehensive Management on Distributed LLM Training}, 
      author={Bohan Zhao and Guang Yang and Shuo Chen and Ruitao Liu and Tingrui Zhang and Yongchao He and Wei Xu},
      year={2025},
      eprint={2507.19845},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      abstract={The rapid escalation in the parameter count of large language models (LLMs) has transformed model training from a single-node endeavor into a highly intricate, cross-node activity. While frameworks such as Megatron-LM successfully integrate tensor (TP), pipeline (PP), and data (DP) parallelism to enable trillion-parameter training, they simultaneously expose practitioners to unprecedented systems-level challenges in performance optimization, diagnosis, and interpretability. MegatronApp is an open-source toolchain expressly designed to meet these challenges. It introduces four orthogonal, yet seamlessly composable modules--MegaScan, MegaFBD, MegaDPP, and MegaScope--that collectively elevate the reliability, efficiency, and transparency of production-scale training. This paper presents the motivation, architecture, and distinctive contributions of each module, and elucidates how their synergistic integration augments the Megatron-LM ecosystem.},
      url={https://arxiv.org/abs/2507.19845}, 
}
@misc{he2025sipipebridgingcpugpuutilization,
      selected={true},
      title={SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference}, 
      author={Yongchao He and Bohan Zhao and Zheng Cao},
      year={2025},
      eprint={2506.22033},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      abstract={As inference workloads for large language models (LLMs) scale to meet growing user demand, pipeline parallelism (PP) has become a widely adopted strategy for multi-GPU deployment, particularly in cross-node setups, to improve key-value (KV) cache capacity and inference throughput. However, PP suffers from inherent inefficiencies caused by three types of execution bubbles-load-imbalance, intra-stage, and inter-stage-which limit pipeline saturation. We present SiPipe, a heterogeneous pipeline design that improves throughput by leveraging underutilized CPU resources to offload auxiliary computation and communication. SiPipe incorporates three key techniques-CPU sampling, a token-safe execution model, and structure-aware transmission-to mitigate pipeline bubbles and improve execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1 times higher throughput, 43% lower per-token latency, and up to 23% higher average GPU utilization compared to the state-of-the-art vLLM under the same PP configuration, demonstrating its generality across LLMs and deployment scenarios.},
      url={https://arxiv.org/abs/2506.22033}, 
      preview={sipipe.pdf}
}
@misc{liu2025heterospecleveragingcontextualheterogeneity,
      selected={true},
      title={HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding}, 
      author={Siran Liu and Yang Ye and Qianchao Zhu and Zane Cao and Yongchao He},
      year={2025},
      eprint={2505.13254},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abstract={Autoregressive decoding inherently limits the inference throughput of Large Language Model (LLM) due to its sequential dependency. Speculative decoding mitigates this by verifying multiple predicted tokens in parallel, but its efficiency remains constrained by what we identify as verification heterogeneity -- the uneven difficulty of verifying different speculative candidates. In practice, a small subset of high-confidence predictions accounts for most successful verifications, yet existing methods treat all candidates uniformly, leading to redundant computation. We present HeteroSpec, a heterogeneity-adaptive speculative decoding framework that allocates verification effort in proportion to candidate uncertainty. HeteroSpec estimates verification complexity using a lightweight entropy-based quantifier, partitions candidates via a data-driven stratification policy, and dynamically tunes speculative depth and pruning thresholds through coordinated optimization. Across five benchmarks and four LLMs, HeteroSpec delivers an average 4.24× decoding speedup over state-of-the-art methods such as EAGLE-3, while preserving exact output distributions. Crucially, HeteroSpec requires no model retraining and remains compatible with other inference optimizations, making it a practical direction for improving speculative decoding efficiency.},
      url={https://arxiv.org/abs/2505.13254}, 
      preview={heterospec.pdf}
}
@INPROCEEDINGS{10188742,
      author={Dou, Songshi and He, Yongchao and Liu, Sen and Wu, Wenfei and Guo, Zehua},
      booktitle={2023 IEEE/ACM 31st International Symposium on Quality of Service (IWQoS)}, 
      title={RateSheriff: Multipath Flow-aware and Resource Efficient Rate Limiter Placement for Data Center Networks}, 
      year={2023},
      volume={},
      number={},
      pages={01-10},
      keywords={Data centers;Limiting;Simulation;Memory management;Switches;Quality of service;TCPIP},
      doi={10.1109/IWQoS57198.2023.10188742},
      url={https://ieeexplore.ieee.org/abstract/document/10188742}
}
@inproceedings{10.1145/3575693.3575708,
      selected={true},
      author = {He, Yongchao and Wu, Wenfei and Le, Yanfang and Liu, Ming and Lao, ChonLam},
      title = {A Generic Service to Provide In-Network Aggregation for Key-Value Streams},
      year = {2023},
      isbn = {9781450399166},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3575693.3575708},
      doi = {10.1145/3575693.3575708},
      abstract = {Key-value stream aggregation is a common operation in distributed systems, which requires intensive computation and network resources. We propose a generic in-network aggregation service for key-value streams, ASK, to accelerate the aggregation operations in diverse distributed applications. ASK is a switch-host co-designed system, where the programmable switch provides a best-effort aggregation service, and the host runs a daemon to interact with applications. ASK makes in-depth optimization tailored to traffic characteristics, hardware restrictions, and network unreliable natures: it vectorizes multiple key-value tuples’ aggregation of one packet in one switch pipeline pass, which improves the per-host’s goodput; it develops a lightweight reliability mechanism for key-value stream’s asynchronous aggregation, which guarantees computation correctness; it designs a hot-key agnostic prioritization for key-skewed workloads, which improves the switch memory utilization. We prototype ASK and use it to support Spark and BytePS. The evaluation shows that ASK could accelerate pure key-value aggregation tasks by up to 155 times and big data jobs by 3-5 times, and be backward compatible with existing INA-empowered distributed training solutions with the same speedup.},
      booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
      pages = {33–47},
      numpages = {15},
      keywords = {Big Data, In-Network Aggregation, Key-Value, P4},
      location = {Vancouver, BC, Canada},
      series = {ASPLOS 2023},
      preview={ASK.pdf}
}
@INPROCEEDINGS{9812884,
      author={He, Yongchao and Wu, Wenfei},
      booktitle={2022 IEEE/ACM 30th International Symposium on Quality of Service (IWQoS)}, 
      title={Consistent and Fine-Grained Rule Update with In-Network Control for Distributed Rate Limiting}, 
      year={2022},
      volume={},
      number={},
      pages={1-10},
      keywords={Wide area networks;Cloud computing;Limiting;Process control;Bandwidth;Quality of service;Control systems},
      doi={10.1109/IWQoS54832.2022.9812884},
      url={https://ieeexplore.ieee.org/document/9812884}
}
@INPROCEEDINGS{9820721,
      author={Huang, Hongyi and Wu, Wenfei and He, Yongchao and Guo, Zehua},
      booktitle={2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
      title={SFP: Service Function Chain Provision on Programmable Switches for Cloud Tenants}, 
      year={2022},
      volume={},
      number={},
      pages={1239-1249},
      keywords={Integer programming;Cloud computing;Runtime;Service function chaining;Pipelines;Prototypes;Process control;NFV;programmable switch},
      doi={10.1109/IPDPS53621.2022.00123},
      url={https://ieeexplore.ieee.org/document/9820721}
}
@INPROCEEDINGS{9488773,
      author={He, Yongchao and Wu, Wenfei and Wen, Xuemin and Li, Haifeng and Yang, Yongqiang},
      booktitle={IEEE INFOCOM 2021 - IEEE Conference on Computer Communications}, 
      title={Scalable On-Switch Rate Limiters for the Cloud}, 
      year={2021},
      volume={},
      number={},
      pages={1-10},
      keywords={Limiting;Scalability;Conferences;Prototypes;Production;Channel allocation;Approximation algorithms},
      doi={10.1109/INFOCOM42981.2021.9488773},
      url={https://dl.acm.org/doi/abs/10.1109/INFOCOM42981.2021.9488773},
      preview={switchrl.pdf}
}
@INPROCEEDINGS{9488734,
      author={Huang, Hongyi and Wu, Wenfei and He, Yongchao and Deng, Bangwen and Zhang, Ying and Xiong, Yongqiang and Chen, Guo and Cui, Yong and Cheng, Peng},
      booktitle={IEEE INFOCOM 2021 - IEEE Conference on Computer Communications}, 
      title={NFD: Using Behavior Models to Develop Cross-Platform Network Functions}, 
      year={2021},
      volume={},
      number={},
      pages={1-10},
      keywords={Biological system modeling;Linux;Conferences;Computational modeling;Ecosystems;Prototypes;Graphics processing units},
      doi={10.1109/INFOCOM42981.2021.9488734},
      url={https://dl.acm.org/doi/10.1145/3342280.3342342},
      preview={nfd.pdf}
}
@inproceedings{10.1145/3342280.3342344,
      author = {He, Yongchao and Wu, Wenfei},
      title = {Fully Functional Rate Limiter Design on Programmable Hardware Switches},
      year = {2019},
      isbn = {9781450368865},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3342280.3342344},
      doi = {10.1145/3342280.3342344},
      booktitle = {Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos},
      pages = {159–160},
      numpages = {2},
      keywords = {Network Function, P4, Rate Limiter},
      location = {Beijing, China},
      series = {SIGCOMM Posters and Demos '19}
}
@INPROCEEDINGS{8885120,
      author={Jiang, Yimin and Cui, Yong and Wu, Wenfei and Xu, Zhe and Gu, Jiahan and Ramakrishnan, K. K. and He, Yongchao and Qian, Xuehai},
      booktitle={2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)}, 
      title={SpeedyBox: Low-Latency NFV Service Chains with Cross-NF Runtime Consolidation}, 
      year={2019},
      volume={},
      number={},
      pages={68-79},
      keywords={Noise measurement;Runtime;Optimization;Redundancy;Network function virtualization;Acceleration;Aggregates;NFV;low-latency;service chain;consolidation},
      doi={10.1109/ICDCS.2019.00016},
      url={https://ieeexplore.ieee.org/document/8885120}
}
